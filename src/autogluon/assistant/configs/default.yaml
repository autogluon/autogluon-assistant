# Tutorial Prompt Generator Configuration

per_execution_timeout: 28800

# MCTS (Monte Carlo Tree Search) parameters
exploration_constant: 1.414    # Controls exploration vs exploitation trade-off in UCT formula (higher = more exploration)
max_debug_depth: 3         # Maximum depth of debug nodes in the search tree
failure_offset: 2             # Number of failures to ignore before applying failure penalty
failure_penalty_weight: 0.5   # Weight of the penalty for failed executions in UCT calculation
initial_root_children: 3      # Maximum number of child nodes from root before considering fully expanded
max_debug_children: 2         # Maximum number of debug child nodes for a single parent node
max_evolve_children: 2        # Maximum number of evolution child nodes for a single parent node

# Data Perception
max_file_group_size_to_show: 5
num_example_files_to_show: 1

max_chars_per_file: 768
num_tutorial_retrievals: 30
max_num_tutorials: 5
max_user_input_length: 2048
max_tutorial_length: 32768
configure_env: false
condense_tutorials: True
use_tutorial_summary: True
continuous_improvement: False
optimize_system_resources: False
cleanup_unused_env: True
enable_meta_prompting: False

# Default LLM Configuration
# For each agent (coder, etc.) you can use a different one
llm: &default_llm
  # Note: bedrock is only supported in limited AWS regions
  #       and requires AWS credentials
  provider: bedrock
  model: "us.anthropic.claude-sonnet-4-5-20250929-v1:0"
  max_tokens: 65535
  proxy_url: null
  temperature: 0.1
  top_p: 0.9
  verbose: True
  multi_turn: False
  template: null
  add_coding_format_instruction: false
  apply_meta_prompting: False

python_coder:
  <<: *default_llm  # Merge llm_config
  multi_turn: True
  apply_meta_prompting: True

bash_coder:
  <<: *default_llm  # Merge llm_config
  multi_turn: True

executer:
  <<: *default_llm  # Merge llm_config
  
meta_prompting:
  <<: *default_llm  # Merge llm_config
  multi_turn: False

reader:
  <<: *default_llm  # Merge llm_config
  details: False

error_analyzer:
  <<: *default_llm  # Merge llm_config

retriever:
  <<: *default_llm  # Merge llm_config

reranker:
  <<: *default_llm  # Merge llm_config
  temperature: 0.
  top_p: 1.

description_file_retriever:
  <<: *default_llm  # Merge llm_config
  temperature: 0.
  top_p: 1.

task_descriptor:
  <<: *default_llm  # Merge llm_config
  max_description_files_length_to_show: 1024
  max_description_files_length_for_summarization: 16384
  apply_meta_prompting: True

tool_selector:
  <<: *default_llm  # Merge llm_config
  temperature: 0.
  top_p: 1.
